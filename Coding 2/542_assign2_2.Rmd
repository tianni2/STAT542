---
title: "542_assign2_2"
author: "Tian Ni"
date: "9/23/2021"
output:
  html_document:
    df_print: paged
---

```{r}
library(glmnet)
library(pls)
set.seed(6659)
```

Load the data

```{r}
myData=read.csv("BostonData2.csv")
myData=myData[,-1]
dim(myData)
X=data.matrix(myData[,-1])
Y=data.matrix(myData[,1])
```

Then we construct those seven procedure to make it easier to read 

```{r}
T=50
n=length(Y)
ntest=round(n*0.25)
sample=sample(n,ntest)
test=myData[sample,]
train=myData[-sample,]
```

```{r}
full=function(train,test){
  full.model=lm(Y~.,data = train)
  y.pred=predict(full.model,newdata=test)
  MSPE=mean((test$Y-y.pred)^2)
  return(MSPE)
}
```

```{r}
## For ridge regression, we need first find the correct range of lambda
cv.out=cv.glmnet(X[-sample,],Y[-sample],alpha=0)
best.lam=cv.out$lambda.min
sum(cv.out$lambda<best.lam)
plot(cv.out)
# Try a lower lambda range to get the lowest MSPE
mylasso.lambda.seq=exp(seq(-10,-2,length.out=100))
cv.out=cv.glmnet(X[-sample,],Y[-sample],alpha=0,lambda = mylasso.lambda.seq) 
plot(cv.out)   
```

Now we found the range of lambda, construct the ridge function

```{r}
ridge=function(xtrain,xtest,ytrain,ytest,lambda.list){
  cv.out=cv.glmnet(xtrain,ytrain,alpha=0,lambda = lambda.list) 
  best.lam=cv.out$lambda.min
  Ytest.pred=predict(cv.out,s=best.lam,newx=xtest)
  ridge.min=mean((ytest-Ytest.pred)^2)
  best.lam=cv.out$lambda.1se
  Ytest.pred=predict(cv.out,s=best.lam,newx=xtest)
  ridge.1se=mean((ytest-Ytest.pred)^2)
  
  return(c(ridge.min,ridge.1se))
}
```

Now we look at the lasso function

```{r}
lasso=function(xtrain,xtest,ytrain,ytest){
  cv.out=cv.glmnet(xtrain,ytrain,alpha=1)
  best.lam=cv.out$lambda.min
  Ytest.pred=predict(cv.out,s=best.lam,newx=xtest)
  lasso.min=mean((ytest-Ytest.pred)^2)
  
  best.lam=cv.out$lambda.1se
  Ytest.pred=predict(cv.out,s=best.lam,newx=xtest)
  lasso.1se=mean((ytest-Ytest.pred)^2)
  
  mylasso.coef=predict(cv.out,s=best.lam,type="coefficients")
  var.sel=row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
  mylasso.refit=lm(Y~.,myData[-sample,c("Y",var.sel)])
  Ytest.pred=predict(mylasso.refit,newdata=myData[sample,])
  lasso.refit=mean((Ytest.pred-ytest)^2)
  
  return(c(lasso.min,lasso.1se,lasso.refit))
}
```

Now we work on the PCR function

```{r}
myPCR=function(train,ytrain,test,ytest){
  mypcr=pcr(Y~., data=train,validation="CV")
  CVerr=RMSEP(mypcr)$val[1, ,]
  adjCVerr=RMSEP(mypcr)$val[2, ,]
  best.ncomp=which.min(CVerr)-1
  if(best.ncomp==0){
    Ytest.pred=mean(myData$Y[-sample])
  }
  else{
    Ytest.pred=predict(mypcr,test,ncomp=best.ncomp)
  }
  pcr=mean((Ytest.pred-ytest)^2)
  return(pcr)
}
```

Run the simulation 50 times

```{r}
MSPE=matrix(rep(NA,350),50,7)
for(t in 1:T){
  sample=sample(n,ntest)
  test=myData[sample,]
  train=myData[-sample,]
  MSPE[t,1]=full(train,test)
  MSPE[t,2:3]=ridge(X[-sample,],X[sample,],Y[-sample,],Y[sample,],mylasso.lambda.seq)
  MSPE[t,4:6]=lasso(X[-sample,],X[sample,],Y[-sample,],Y[sample,])
  MSPE[t,7]=myPCR(train,train$Y,test,test$Y)
}
colnames(MSPE)=c("full","ridge.min","ridge.1se","lasso.min","lasso.1se","lasso.refit","pcr")
boxplot(MSPE,col=c("orange",rep("blue",2),rep("purple",3),"darkgrey"))
```

From the boxplot we can see that, almost all the MSPEs produced by these seven procedures are under 0.05 except three "outliers" in full model and PCR method. Moreover, lasso's MSPEs are a little bit higher than MSPE of ridge averagely. In general, for both lasso and ridge, the "lambda.min" always gives smaller MSPE than "lambda.1se". The ridge with lambda.min gives the smallest MSPE and lasso with 1se lambda gives the greatest MSPE on average. 


Try on another data
--

Load the newdata

```{r}
myData=read.csv("BostonData3.csv")
myData=myData[,-1]
dim(myData)
X=data.matrix(myData[,-1])
Y=data.matrix(myData[,1])
MSPE.new=matrix(rep(NA,300),50,6)

## For ridge regression, we need first find the correct range of lambda
cv.out=cv.glmnet(X[-sample,],Y[-sample],alpha=0)
best.lam=cv.out$lambda.min
sum(cv.out$lambda<best.lam)
plot(cv.out)
# Try a lower lambda range to get the lowest MSPE
mylasso.lambda.seq=exp(seq(-6,2,length.out=100))
cv.out=cv.glmnet(X[-sample,],Y[-sample],alpha=0,lambda = mylasso.lambda.seq) 
plot(cv.out) 

for(t in 1:T){
  sample=sample(n,ntest)
  test=myData[sample,]
  train=myData[-sample,]
  MSPE.new[t,1:2]=ridge(X[-sample,],X[sample,],Y[-sample,],Y[sample,],mylasso.lambda.seq)
  MSPE.new[t,3:5]=lasso(X[-sample,],X[sample,],Y[-sample,],Y[sample,])
  MSPE.new[t,6]=myPCR(train,train$Y,test,test$Y)
}
colnames(MSPE.new)=c("ridge.min","ridge.1se","lasso.min","lasso.1se","lasso.refit","pcr")
boxplot(MSPE.new,col=c(rep("blue",2),rep("purple",3),"darkgrey"))
```

From the boxplot we could obviously catch that, PCR method gives the greatest MSPE and ridge with lambda.1se gives the second largest MSPE. On average, MSPE of lasso is lower that the MSPE of ridge, which is exactly the converse of what we had in Boston 2.

